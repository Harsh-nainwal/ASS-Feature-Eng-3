{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fdb3c1-0171-49d3-aefb-974549d03b94",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features in a dataset into a common range. The goal is to scale the values of the features so that they fall within a specific range, typically between 0 and 1. This can be particularly useful when the features have different ranges and magnitudes, as it ensures that all features contribute equally to the analysis or modeling process.\n",
    "Formula for it is :\n",
    "    Xs = (x - Xmin)/(Xmax-Xmin)\n",
    "    Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset containing two features: \"Age\" and \"Income.\" The \"Age\" feature ranges from 18 to 80 years, while the \"Income\" feature ranges from $20000 to $100,000. Since these features have different ranges, applying machine learning algorithms directly might give more weight to the \"Income\" feature due to its larger magnitude, potentially affecting the model's performance.\n",
    "\n",
    "By using Min-Max scaling, you can bring both features to a common range of [0,1]. Let's say you want to scale the \"Age\" and \"Income\" features for a particular data point:\n",
    "\n",
    "Age: 35 years\n",
    "Income: $60,000\n",
    " after Min-Max scaling, the scaled values would be approximately:\n",
    "\n",
    "Scaled Age: 0.2632\n",
    "Scaled Income: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92530fcf-6830-4b22-88a4-3c1b0c6aafa2",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans:-\n",
    "The Unit Vector technique, also known as \"Unit Norm\" or \"Normalization,\" is a feature scaling method that scales the values of a feature vector to have a unit norm (length) in a vector space. In other words, it scales the vector so that its Euclidean norm (magnitude) becomes 1\n",
    "\n",
    "Unit Vector scaling is different from Min-Max scaling in that it focuses on the direction of the feature vector rather than its range of values. Min-Max scaling brings the values of the feature vector into a specific range (usually [0, 1]), while Unit Vector scaling ensures that the vector points in the same direction but has a magnitude of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd1391-07ea-4313-9e2a-ce2ef5dee3cd",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502de0f5-d224-43f2-82b4-a1f5d379dd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949</td>\n",
       "      <td>Jan</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949</td>\n",
       "      <td>Feb</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>Mar</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949</td>\n",
       "      <td>Apr</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949</td>\n",
       "      <td>May</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1960</td>\n",
       "      <td>Aug</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1960</td>\n",
       "      <td>Sep</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1960</td>\n",
       "      <td>Oct</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1960</td>\n",
       "      <td>Nov</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1960</td>\n",
       "      <td>Dec</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year month  passengers\n",
       "0    1949   Jan         112\n",
       "1    1949   Feb         118\n",
       "2    1949   Mar         132\n",
       "3    1949   Apr         129\n",
       "4    1949   May         121\n",
       "..    ...   ...         ...\n",
       "139  1960   Aug         606\n",
       "140  1960   Sep         508\n",
       "141  1960   Oct         461\n",
       "142  1960   Nov         390\n",
       "143  1960   Dec         432\n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "df =sns.load_dataset(\"flights\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09fecc0-1753-4907-aafc-d50524537924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef254b3-9363-47ca-be24-d97ec9b347a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'passengers'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ab4165-f76c-4e4f-9d11-41b64db8526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(df[[\"year\"]])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89966d4a-1b60-4d94-baa8-4d6b4201a667",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while retaining as much of the original variance as possible. It achieves this by finding a new set of orthogonal axes, called principal components, in the data space. These principal components capture the directions of maximum variance in the data, allowing for a more compact and informative representation.\n",
    "\n",
    " how PCA works:\n",
    "\n",
    "Compute the Mean: Calculate the mean vector of the data along each feature dimension.\n",
    "\n",
    "Center the Data: Subtract the mean vector from each data point. This step ensures that the data is centered around the origin.\n",
    "\n",
    "Compute Covariance Matrix: Compute the covariance matrix of the centered data. The covariance matrix shows how different features vary together.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each component.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors based on their corresponding eigenvalues in decreasing order. This step allows you to prioritize the most important components.\n",
    "\n",
    "Select Principal Components: Choose the top k  eigenvectors to retain. These k eigenvectors form the new lower-dimensional space.\n",
    "\n",
    "Project Data: Transform the original data onto the new lower-dimensional space defined by the selected principal componentks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96469c9-33e0-4dfd-b80c-3d3e4e8d2109",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans:-\n",
    "PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to transform the original features into a new set of features (principal components) that capture the most important information in the data while reducing its dimensionality. Feature extraction aims to create a more compact and representative feature space that can improve the efficiency and effectiveness of various data analysis and modeling tasks.\n",
    "\n",
    " how PCA can be used for feature extraction:\n",
    "\n",
    "Original Feature Space: You start with a dataset containing a high number of original features (dimensions). These features might be highly correlated, noisy, or redundant.\n",
    "\n",
    "Apply PCA: You apply PCA to the dataset, which involves finding the principal components that capture the directions of maximum variance in the data. These principal components are linear combinations of the original features.\n",
    "\n",
    "Select Components: You select a subset of the principal components based on the amount of variance they explain or other criteria. These selected components become the new features in the transformed feature space.\n",
    "\n",
    "New Feature Space: The new feature space has a reduced dimensionality compared to the original feature space. It retains most of the important information from the original data but in a more compact form.\n",
    "\n",
    "Use in Analysis/Modeling: The transformed data in the new feature space can be used for various tasks such as visualization, clustering, classification, regression, and other data analysis or modeling techniques.\n",
    "\n",
    "Here's an example to illustrate using PCA for feature extraction:\n",
    "\n",
    "Suppose you have a dataset of images of handwritten digits, each represented as a 64-pixel grayscale image (8x8 pixels). You want to perform digit recognition using a machine learning algorithm. However, the high dimensionality of the images can lead to computational challenges and overfitting.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Each image is represented as a vector of 64 pixel values.\n",
    "You can use PCA for feature extraction as follows:\n",
    "\n",
    "Apply PCA: Apply PCA to the dataset of images. The principal components will be computed based on the pixel values of the images.\n",
    "\n",
    "Select Components: Decide how many principal components to retain. For instance, you might choose to retain the top 20 components, which capture a significant portion of the variance.\n",
    "\n",
    "New Feature Space: Each image is now represented by a 20-dimensional vector (the 20 selected principal components).\n",
    "\n",
    "Use in Digit Recognition: You can now use the reduced-dimensional representation for digit recognition. Train a machine learning algorithm (such as a classifier) using the transformed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20194916-a865-46bd-bfab-7919383ebc54",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "\n",
    "Ans:-\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess the data and ensure that the features are on a consistent scale. This helps prevent certain features from dominating the recommendation process due to their larger magnitudes. Here's how you would use Min-Max scaling for the features like price, rating, and delivery time:\n",
    "\n",
    "Understand the Features: First, it's important to understand the range and distribution of each feature. Look at the minimum and maximum values for price, rating, and delivery time.\n",
    "\n",
    "Apply Min-Max Scaling: Apply Min-Max scaling to each feature separately. The goal is to transform the values of each feature to a common range, typically [0, 1].\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    " Xs = X - Xmin/Xmax-Xmin\n",
    " \n",
    "For the \"price\" feature: Let's say the minimum price is $5 and the maximum price is $30. Apply the Min-Max scaling formula to each price value in the dataset to scale them to the range [0, 1].\n",
    "\n",
    "For the \"rating\" feature: If ratings range from 1 to 5, apply Min-Max scaling to bring them to the range [0, 1].\n",
    "\n",
    "For the \"delivery time\" feature: If delivery times range from 20 minutes to 60 minutes, apply Min-Max scaling to normalize them to the range [0, 1].\n",
    "\n",
    "Transform the Data: After applying Min-Max scaling, you will have new scaled values for each feature. These scaled values are now in the [0, 1] range and are ready to be used for the recommendation system.\n",
    "\n",
    "Use in Recommendation System: The scaled features can now be used in your recommendation system algorithm. The scaled features ensure that no single feature dominates the recommendation process due to its magnitude.\n",
    "\n",
    "For example, if you were to use the recommendation system to suggest food items to users, the scaled features would be used to calculate similarity scores between items and users' preferences. These similarity scores would then guide the recommendations, ensuring that the recommendations consider all relevant features (price, rating, delivery time) in a balanced manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f820e2-85d6-493b-9686-cfd93e70b1fb",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans:-\n",
    "\n",
    "In the context of building a model to predict stock prices, you can use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset, which can help mitigate the \"curse of dimensionality\" and potentially improve the performance of your predictive model. Here's how you would use PCA for dimensionality reduction in this scenario:\n",
    "\n",
    "Feature Selection and Understanding: Begin by carefully selecting the features that are relevant to predicting stock prices. These features might include company-specific financial metrics (e.g., revenue, earnings, debt) as well as broader market trends (e.g., interest rates, market indices).\n",
    "\n",
    "Data Preprocessing: Standardize or normalize the selected features to ensure that they are on a similar scale. This step is important because PCA is sensitive to the scale of the features.\n",
    "\n",
    "Apply PCA: Apply PCA to the standardized or normalized feature matrix. The goal is to transform the original high-dimensional feature space into a new lower-dimensional space defined by the principal components.\n",
    "\n",
    "Determine Number of Components: Determine the number of principal components to retain. This can be done based on the explained variance ratio, which indicates how much of the total variance is explained by each component. You can choose to retain a sufficient number of components to capture a significant portion of the variance (e.g., 95% or more).\n",
    "\n",
    "Transform Data: Transform the original feature matrix using the selected principal components. This transformation reduces the dimensionality of the data while retaining most of the important information.\n",
    "\n",
    "Model Training and Evaluation: Use the transformed data as input to train your predictive model. You can use various machine learning algorithms such as regression, time series models, or neural networks. Evaluate the model's performance using appropriate metrics and techniques.\n",
    "\n",
    "Interpretation: Although the principal components themselves might not be directly interpretable, you can still gain insights by examining the relationship between the original features and the principal components. The components with larger loadings for specific original features might indicate the features' importance in capturing certain patterns in the data.\n",
    "\n",
    "By using PCA for dimensionality reduction, you achieve several benefits:\n",
    "\n",
    "Reduced Complexity: The lower-dimensional space reduces computational complexity and memory usage, making model training more efficient.\n",
    "\n",
    "Less Overfitting: A lower-dimensional feature space reduces the risk of overfitting, as the model has fewer parameters to learn from the data.\n",
    "\n",
    "Multicollinearity Mitigation: If there's multicollinearity (high correlation) among features, PCA can help remove or reduce this issue.\n",
    "\n",
    "Noise Reduction: PCA tends to emphasize components that capture the most significant variation in the data, potentially filtering out noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212831d6-1e45-4636-85c8-eb9ce668d79f",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d44f93-2350-4c0a-97c2-845564d12d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: [ 1  5 10 15 20]\n",
      "Scaled dataset: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate min and max values\n",
    "X_min = np.min(data)\n",
    "X_max = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = ((data - X_min) / (X_max - X_min)) * 2 - 1\n",
    "\n",
    "print(\"Original dataset:\", data)\n",
    "print(\"Scaled dataset:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6415cb0-b734-45cd-b74f-15de0d05d6eb",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The decision of how many principal components to retain during feature extraction using PCA depends on the goals of your analysis, the nature of the data, and the trade-off between dimensionality reduction and information preservation. Here are some considerations to help you decide how many principal components to retain:\n",
    "\n",
    "Explained Variance: One common approach is to look at the cumulative explained variance ratio. This ratio indicates how much of the total variance in the data is explained by each principal component and its preceding components. Retaining enough principal components to capture a high percentage of the total variance (e.g., 95% or more) ensures that you're preserving most of the important information.\n",
    "\n",
    "Visualization: If you plan to visualize the data, retaining 2 or 3 principal components can allow you to plot the data in a lower-dimensional space while still maintaining some visual separation between data points.\n",
    "\n",
    "Model Performance: If your goal is to use the reduced-dimensional data for modeling (e.g., classification or regression), you can experiment with different numbers of principal components and see how the model's performance changes. Keep in mind that adding too many components may lead to overfitting.\n",
    "\n",
    "Interpretability: Retaining a smaller number of principal components makes it easier to interpret the resulting features and understand the underlying patterns. High-dimensional spaces can become challenging to interpret.\n",
    "\n",
    "Curse of Dimensionality: Reducing the dimensionality too much may cause you to lose critical information, while not reducing it enough might not provide significant benefits in terms of computational efficiency or model performance.\n",
    "\n",
    "Feature Correlation: Consider whether there are strong correlations among the original features. Highly correlated features might be well represented by a smaller number of principal components.\n",
    "\n",
    "Domain Knowledge: If you have domain knowledge about which features are most relevant, you might prioritize retaining components that align with those features.\n",
    "\n",
    "Ultimately, there's no one-size-fits-all answer for how many principal components to retain. It's a balance between reducing dimensionality, preserving information, and maintaining the interpretability of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec1f3b-18c7-4ba5-ac13-84e2fb72e8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
